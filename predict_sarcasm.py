# -*- coding: utf-8 -*-
"""
è®½åˆºè¯†åˆ«æ¨¡å‹é¢„æµ‹è„šæœ¬ - æœ€ç»ˆä¿®å¤ç‰ˆ
å®Œå…¨è§£å†³åµŒå…¥å±‚å°ºå¯¸ä¸åŒ¹é…é—®é¢˜
"""

import os
import torch
import numpy as np
import json
import pickle
from dataUtils import DataManager
from bridgeModel import bridgeModel


class ModelConfig:
    """æ¨¡å‹é…ç½®ç±»ï¼Œä¸ä½ çš„è®­ç»ƒé…ç½®ä¿æŒä¸€è‡´"""
    def __init__(self):
        # åŸºç¡€é…ç½®
        self.seed = 2021
        self.name_dataset = "IAC2"
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # æ¨¡å‹ç»“æ„å‚æ•°ï¼ˆè¿™äº›å°†è¢«è‡ªåŠ¨æ£€æµ‹è¦†ç›–ï¼‰
        self.voc_size = 30000
        self.dim_input = 300
        self.dim_hidden = 256
        self.dim_bert = 768
        self.n_layers = 1  # å°†è¢«è‡ªåŠ¨æ£€æµ‹è¦†ç›–
        self.n_class = 2
        self.bidirectional = 1  # å°†è¢«è‡ªåŠ¨æ£€æµ‹è¦†ç›–
        self.rnn_type = "LSTM"
        self.max_length_sen = 100

        # è®­ç»ƒå‚æ•°
        self.learning_rate = 0.001
        self.lr_word_vector = 1e-4
        self.lr_bert = 5e-05
        self.weight_decay = 0
        self.batch_size = 1
        self.iter_num = 32 * 150
        self.per_checkpoint = 32
        self.optim_type = "Adam"

        # Dropoutå‚æ•°
        self.embed_dropout_rate = 0.5
        self.cell_dropout_rate = 0.5
        self.final_dropout_rate = 0.5
        self.linear_dropout_rate = 0.1

        # å…¶ä»–å‚æ•°
        self.t_sne = 0
        self.lambda1 = 0.5
        self.multi_dim = 20
        self.tokenizer = "spacy"
        self.margin = 0.5
        self.supcon = 1
        self.name_model = "dualbilstm"

        # è·¯å¾„å‚æ•°
        self.data_dir = "IAC2/spacy/"
        self.path_wordvec = "glove.840B.300d.txt"
        self.predict_dir = "./predict/"
        self.model_dir = "./models/"
        self.save_model = 0
        self.predict = 0


def analyze_model_structure(model_path, device):
    """åˆ†ææ¨¡å‹æ–‡ä»¶ç»“æ„ï¼Œæ£€æµ‹å…³é”®å‚æ•°"""
    try:
        state_dict = torch.load(model_path, map_location=device)
        print("=" * 60)
        print("æ¨¡å‹ç»“æ„åˆ†ææŠ¥å‘Š")
        print("=" * 60)
        
        # æ˜¾ç¤ºå‰20ä¸ªå…³é”®é”®
        print("\nå…³é”®å‚æ•°é”®:")
        all_keys = sorted(state_dict.keys())
        for key in all_keys[:20]:
            print(f"  {key}: {state_dict[key].shape}")
        
        if len(all_keys) > 20:
            print(f"  ... å’Œ {len(all_keys) - 20} ä¸ªæ›´å¤šé”®")

        # åˆ†æå…³é”®å‚æ•°
        embed_keys = [k for k in all_keys if 'embed.weight' in k]
        rnn_keys = [k for k in all_keys if 'rnn' in k and 'weight_ih_l' in k]
        
        # æ£€æµ‹è¯è¡¨å¤§å°
        vocab_size = None
        if embed_keys:
            vocab_size = state_dict[embed_keys[0]].shape[0]
            print(f"  - åµŒå…¥å±‚è¯è¡¨å¤§å°: {vocab_size}")

        # æ£€æµ‹RNNå±‚æ•°
        layer_numbers = set()
        for key in rnn_keys:
            parts = key.split('l')
            if len(parts) > 1:
                layer_num_str = parts[-1].split('_')[0]
                if layer_num_str.isdigit():
                    layer_numbers.add(int(layer_num_str))
        
        n_layers = max(layer_numbers) + 1 if layer_numbers else 1
        print(f"  - RNNå±‚æ•°: {n_layers} (æ£€æµ‹åˆ°å±‚å·: {sorted(layer_numbers)})")

        # æ£€æµ‹åŒå‘æ€§
        bidirectional = any('reverse' in key for key in all_keys)
        print(f"  - åŒå‘RNN: {bidirectional}")

        # æ£€æµ‹éšè—å±‚ç»´åº¦
        hidden_dim = None
        for key in rnn_keys:
            if 'weight_hh_l0' in key:
                hidden_dim = state_dict[key].shape[1]
                break
        if hidden_dim:
            print(f"  - éšè—å±‚ç»´åº¦: {hidden_dim}")

        print(f"  - æ€»å‚æ•°é”®æ•°: {len(all_keys)}")
        print(f"  - RNNç›¸å…³é”®: {len(rnn_keys)}")
        print(f"  - åµŒå…¥å±‚é”®: {len(embed_keys)}")
        
        return state_dict, vocab_size, n_layers, bidirectional, hidden_dim
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹ç»“æ„åˆ†æå¤±è´¥: {str(e)}")
        raise


def create_embedding_from_model(state_dict, target_vocab_size, embed_dim):
    """ä»æ¨¡å‹çŠ¶æ€å­—å…¸ä¸­æå–åµŒå…¥å±‚æƒé‡"""
    print("  - ä»æ¨¡å‹çŠ¶æ€å­—å…¸æå–åµŒå…¥å±‚æƒé‡...")
    
    # æŸ¥æ‰¾åµŒå…¥å±‚æƒé‡
    embed_weights = None
    for key in state_dict:
        if 'embed.weight' in key:
            embed_weights = state_dict[key].cpu().numpy()
            print(f"  - æ‰¾åˆ°åµŒå…¥å±‚: {key}, å½¢çŠ¶: {embed_weights.shape}")
            break
    
    if embed_weights is None:
        raise ValueError("åœ¨æ¨¡å‹çŠ¶æ€å­—å…¸ä¸­æ‰¾ä¸åˆ°åµŒå…¥å±‚æƒé‡")
    
    # æ£€æŸ¥å°ºå¯¸å¹¶è°ƒæ•´
    if embed_weights.shape[0] == target_vocab_size:
        print(f"  - åµŒå…¥å±‚å°ºå¯¸åŒ¹é…: {embed_weights.shape}")
        return embed_weights
    elif embed_weights.shape[0] < target_vocab_size:
        # è¡¥å……éšæœºå‘é‡
        pad_size = target_vocab_size - embed_weights.shape[0]
        pad_embed = np.random.normal(0, 0.01, (pad_size, embed_dim)).astype(np.float32)
        new_embed = np.vstack([embed_weights, pad_embed])
        print(f"  - åµŒå…¥å±‚è¡¥å……: {embed_weights.shape} -> {new_embed.shape}")
        return new_embed
    else:
        # æˆªæ–­
        new_embed = embed_weights[:target_vocab_size]
        print(f"  - åµŒå…¥å±‚æˆªæ–­: {embed_weights.shape} -> {new_embed.shape}")
        return new_embed


def create_compatible_model(config, vocab, embed, state_dict):
    """åˆ›å»ºä¸ä¿å­˜çš„state_dictå…¼å®¹çš„æ¨¡å‹"""
    print("  - åˆ›å»ºå…¼å®¹æ¨¡å‹...")
    
    # ä½¿ç”¨æ£€æµ‹åˆ°çš„é…ç½®åˆ›å»ºæ¨¡å‹
    model = bridgeModel(config, vocab=vocab, embed=embed)
    
    # è·å–å½“å‰æ¨¡å‹çš„state_dict
    current_state_dict = model.state_dict()
    
    # åˆ›å»ºæ–°çš„state_dictï¼ŒåªåŠ è½½åŒ¹é…çš„é”®
    new_state_dict = {}
    matched_keys = 0
    shape_mismatch_keys = 0
    missing_keys = 0
    
    print("  - å¼€å§‹æƒé‡åŒ¹é…...")
    for key, value in state_dict.items():
        if key in current_state_dict:
            if current_state_dict[key].shape == value.shape:
                new_state_dict[key] = value
                matched_keys += 1
            else:
                # åªæ˜¾ç¤ºé‡è¦çš„å½¢çŠ¶ä¸åŒ¹é…
                if any(pattern in key for pattern in ['rnn', 'embed', 'linear', 'dense']):
                    print(f"    âš ï¸ å½¢çŠ¶ä¸åŒ¹é…: {key}")
                    print(f"      æœŸæœ›: {current_state_dict[key].shape}, å®é™…: {value.shape}")
                shape_mismatch_keys += 1
        else:
            missing_keys += 1
            # åªæ˜¾ç¤ºé‡è¦çš„ç¼ºå¤±é”®
            if any(pattern in key for pattern in ['rnn', 'embed', 'linear', 'dense']):
                print(f"    âŒ é”®ä¸å­˜åœ¨: {key}")
    
    print(f"  - åŒ¹é…ç»“æœ: {matched_keys}ä¸ªåŒ¹é…, {shape_mismatch_keys}ä¸ªå½¢çŠ¶ä¸åŒ¹é…, {missing_keys}ä¸ªç¼ºå¤±")
    
    if matched_keys > 0:
        # åŠ è½½åŒ¹é…çš„æƒé‡
        model.load_state_dict(new_state_dict, strict=False)
        print("  - âœ… å…¼å®¹æ¨¡å¼åŠ è½½æˆåŠŸ")
        return model
    else:
        print("  - âŒ æ²¡æœ‰åŒ¹é…çš„æƒé‡é”®ï¼Œæ— æ³•åŠ è½½æ¨¡å‹")
        return None


def robust_predict(model_path, test_path, config):
    """
    å¥å£®çš„é¢„æµ‹å‡½æ•° - ä¿®å¤åµŒå…¥å±‚å°ºå¯¸é—®é¢˜
    """
    print("=" * 60)
    print("å¼€å§‹å¥å£®é¢„æµ‹æµç¨‹ - ä¿®å¤ç‰ˆ")
    print("=" * 60)
    
    try:
        # æ­¥éª¤1: æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        if not os.path.exists(test_path):
            raise FileNotFoundError(f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_path}")

        # æ­¥éª¤2: åˆ†ææ¨¡å‹ç»“æ„
        print("[1/7] ğŸ” åˆ†ææ¨¡å‹ç»“æ„...")
        state_dict, model_vocab_size, n_layers, bidirectional, hidden_dim = analyze_model_structure(model_path, config.device)
        
        # æ›´æ–°é…ç½®å‚æ•°
        config.n_layers = n_layers
        config.bidirectional = 1 if bidirectional else 0
        if hidden_dim:
            config.dim_hidden = hidden_dim
        
        print(f"  - åº”ç”¨æ£€æµ‹åˆ°çš„å‚æ•°:")
        print(f"     å±‚æ•°: {n_layers}")
        print(f"     åŒå‘: {bidirectional}")
        print(f"     éšè—å±‚: {hidden_dim}")
        print(f"     è¯è¡¨å¤§å°: {model_vocab_size}")

        # æ­¥éª¤3: åˆå§‹åŒ– DataManager
        print("[2/7] ğŸ“š åˆå§‹åŒ– DataManager...")
        datamanager = DataManager(config)
        
        # æ­¥éª¤4: æ„å»ºè¯è¡¨ï¼ˆä½†ä¸ä½¿ç”¨ç¼“å­˜çš„åµŒå…¥çŸ©é˜µï¼‰
        print("[3/7] ğŸ”¤ æ„å»ºè¯è¡¨...")
        try:
            train_data = datamanager.load_data(config.data_dir, 'train.txt')
            
            # æ‰‹åŠ¨æ„å»ºè¯è¡¨ï¼Œé¿å…ä½¿ç”¨ç¼“å­˜çš„åµŒå…¥çŸ©é˜µ
            print("  - æ‰‹åŠ¨æ„å»ºè¯è¡¨...")
            vocab = []
            vocab_dict = {}
            
            # ä»è®­ç»ƒæ•°æ®æ”¶é›†è¯æ±‡
            word_counts = {}
            for item in train_data:
                for word in item['content']:
                    word_counts[word] = word_counts.get(word, 0) + 1
            
            # æ’åºå¹¶æˆªæ–­
            sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
            vocab = [word for word, count in sorted_words[:config.voc_size]]
            
            # æ·»åŠ ç‰¹æ®Šæ ‡è®°
            if '<unk>' not in vocab:
                vocab.insert(0, '<unk>')
            if '<pad>' not in vocab:
                vocab.insert(0, '<pad>')
            
            # åˆ›å»ºè¯æ±‡å­—å…¸
            vocab_dict = {word: idx for idx, word in enumerate(vocab)}
            print(f"  - æˆåŠŸæ„å»ºè¯è¡¨: {len(vocab)} ä¸ªè¯")
            
        except Exception as e:
            print(f"  - âš ï¸ æ„å»ºè¯è¡¨å¤±è´¥: {e}")
            print("  - ä½¿ç”¨æ¨¡å‹ä¸­çš„è¯è¡¨å¤§å°åˆ›å»ºåŸºç¡€è¯è¡¨...")
            # åˆ›å»ºåŸºç¡€è¯è¡¨
            vocab = [f"word_{i}" for i in range(min(20000, model_vocab_size))]
            vocab_dict = {word: idx for idx, word in enumerate(vocab)}

        # æ­¥éª¤5: ä»æ¨¡å‹çŠ¶æ€å­—å…¸åˆ›å»ºåµŒå…¥çŸ©é˜µ
        print("[4/7] ğŸ¯ åˆ›å»ºåµŒå…¥çŸ©é˜µ...")
        embed = create_embedding_from_model(state_dict, model_vocab_size, config.dim_input)
        
        # æ­¥éª¤6: è°ƒæ•´è¯è¡¨å°ºå¯¸åŒ¹é…åµŒå…¥çŸ©é˜µ
        print("[5/7] ğŸ“ è°ƒæ•´è¯è¡¨åŒ¹é…åµŒå…¥çŸ©é˜µ...")
        if len(vocab) != model_vocab_size:
            print(f"  - è¯è¡¨å°ºå¯¸ä¸åŒ¹é…: å½“å‰{len(vocab)} vs éœ€è¦{model_vocab_size}")
            
            if len(vocab) < model_vocab_size:
                # è¡¥å……è¯è¡¨
                pad_size = model_vocab_size - len(vocab)
                print(f"  - è¡¥å…… {pad_size} ä¸ªè¯")
                
                for i in range(pad_size):
                    word = f"[PAD_{i}]"
                    vocab.append(word)
                    vocab_dict[word] = len(vocab) - 1
            else:
                # æˆªæ–­è¯è¡¨
                print(f"  - æˆªæ–­åˆ° {model_vocab_size} ä¸ªè¯")
                vocab = vocab[:model_vocab_size]
                vocab_dict = {word: idx for idx, word in enumerate(vocab)}
        
        print(f"  - æœ€ç»ˆå°ºå¯¸: è¯è¡¨{len(vocab)}, åµŒå…¥{embed.shape}")

        # æ­¥éª¤7: åˆ›å»ºå’ŒåŠ è½½æ¨¡å‹
        print("[6/7] ğŸ¤– åˆ›å»ºå’ŒåŠ è½½æ¨¡å‹...")
        
        # ç›´æ¥ä½¿ç”¨å…¼å®¹æ¨¡å¼åŠ è½½ï¼ˆé¿å…ä¸¥æ ¼æ¨¡å¼é”™è¯¯ï¼‰
        model = bridgeModel(config, vocab=vocab, embed=embed)
        print("  - ä½¿ç”¨å…¼å®¹æ¨¡å¼åŠ è½½...")
        model = create_compatible_model(config, vocab, embed, state_dict)
        
        if model is None:
            raise RuntimeError("å…¼å®¹æ¨¡å¼åŠ è½½å¤±è´¥")

        model.to(config.device)
        model.eval()

        # æ­¥éª¤8: åŠ è½½æµ‹è¯•æ•°æ®
        print("[7/7] ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®...")
        test_data = datamanager.load_data(config.data_dir, 'test.txt')
        print(f"  - åŠ è½½ {len(test_data)} ä¸ªæµ‹è¯•æ ·æœ¬")

        # æ­¥éª¤9: æ‰§è¡Œé¢„æµ‹
        print("ğŸš€ å¼€å§‹é¢„æµ‹...")
        results = []
        successful_predictions = 0
        
        for i, item in enumerate(test_data, 1):
            try:
                # ä½¿ç”¨ DataManager çš„ gen_batched_data æ–¹æ³•
                batched_data = datamanager.gen_batched_data([item])
                
                with torch.no_grad():
                    _, probs, _, _ = model.stepTrain(batched_data, inference=True)
                
                # æå–é¢„æµ‹æ¦‚ç‡
                if probs is not None and len(probs) > 0:
                    sarcasm_prob = float(probs[0][1])
                    
                    result = {
                        "sentence": item.get('origin', 'Unknown'),
                        "probability": round(sarcasm_prob, 4),
                        "is_sarcastic": sarcasm_prob > 0.5,
                        "true_label": item.get('sarcasm', -1)
                    }
                    successful_predictions += 1
                else:
                    result = {
                        "sentence": item.get('origin', 'Unknown'),
                        "error": "æ¨¡å‹è¿”å›ç©ºæ¦‚ç‡",
                        "probability": 0.5,
                        "is_sarcastic": False,
                        "true_label": item.get('sarcasm', -1)
                    }
                
                results.append(result)
                
            except Exception as e:
                result = {
                    "sentence": item.get('origin', 'Unknown'),
                    "error": str(e),
                    "probability": 0.5,
                    "is_sarcastic": False,
                    "true_label": item.get('sarcasm', -1)
                }
                results.append(result)
            
            # è¿›åº¦æŠ¥å‘Š
            if i % 100 == 0 or i == len(test_data):
                print(f"  - å·²å¤„ç† {i}/{len(test_data)} ä¸ªæ ·æœ¬")

        # ç»Ÿè®¡ç»“æœ
        failed = len(results) - successful_predictions
        sarcastic_count = sum(1 for r in results if r.get('is_sarcastic', False))
        
        print(f"  - é¢„æµ‹ç»Ÿè®¡: æˆåŠŸ{successful_predictions}, å¤±è´¥{failed}, è®½åˆºå¥{sarcastic_count}")

        return results, datamanager

    except Exception as e:
        print(f"âŒ é¢„æµ‹è¿‡ç¨‹å¤±è´¥: {str(e)}")
        raise


def save_predictions(results, output_file="sarcasm_predictions_fixed.txt"):
    """ä¿å­˜é¢„æµ‹ç»“æœ"""
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("è®½åˆºè¯†åˆ«é¢„æµ‹ç»“æœï¼ˆä¿®å¤ç‰ˆï¼‰\n")
        f.write("=" * 60 + "\n\n")
        
        for idx, res in enumerate(results, 1):
            f.write(f"æ ·æœ¬ {idx}:\n")
            f.write(f"åŸæ–‡: {res['sentence']}\n")
            
            if "error" in res:
                f.write(f"çŠ¶æ€: âŒ é¢„æµ‹å¤±è´¥\n")
                f.write(f"é”™è¯¯ä¿¡æ¯: {res['error']}\n")
            else:
                f.write(f"çŠ¶æ€: âœ… é¢„æµ‹æˆåŠŸ\n")
                f.write(f"è®½åˆºæ¦‚ç‡: {res['probability']:.4f}\n")
                f.write(f"é¢„æµ‹ç»“æœ: {'è®½åˆº' if res['is_sarcastic'] else 'éè®½åˆº'}\n")
                if 'true_label' in res and res['true_label'] != -1:
                    true_label = 'è®½åˆº' if res['true_label'] == 1 else 'éè®½åˆº'
                    f.write(f"çœŸå®æ ‡ç­¾: {true_label}\n")
                    correct = res['is_sarcastic'] == (res['true_label'] == 1)
                    f.write(f"é¢„æµ‹æ­£ç¡®: {'æ˜¯' if correct else 'å¦'}\n")
            
            f.write("-" * 50 + "\n")
    
    print(f"ğŸ’¾ ç»“æœä¿å­˜è‡³: {output_file}")


if __name__ == "__main__":
    # é…ç½®å’Œè·¯å¾„
    config = ModelConfig()
    MODEL_PATH = "models/IAC2_dualbilstm/IAC2dualbilstm_best/model69.pth"
    TEST_PATH = "IAC2/spacy/test.txt"
    
    print("ğŸ¯ è®½åˆºè¯†åˆ«æ¨¡å‹é¢„æµ‹ - ä¿®å¤ç‰ˆ")
    print(f"æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
    print(f"æµ‹è¯•æ•°æ®: {TEST_PATH}")
    print(f"è®¾å¤‡: {config.device}")
    print()
    
    try:
        # æ‰§è¡Œå¥å£®é¢„æµ‹
        results, datamanager = robust_predict(MODEL_PATH, TEST_PATH, config)
        
        # ä¿å­˜ç»“æœ
        save_predictions(results)
        
        # æœ€ç»ˆæŠ¥å‘Š
        successful = sum(1 for r in results if 'error' not in r)
        sarcastic = sum(1 for r in results if r.get('is_sarcastic', False))
        
        print("\n" + "=" * 60)
        print("ğŸ‰ é¢„æµ‹å®Œæˆ!")
        print(f"âœ… æˆåŠŸé¢„æµ‹: {successful}/{len(results)}")
        print(f"ğŸ˜Š è®½åˆºæ ·æœ¬: {sarcastic}")
        print(f"ğŸ˜ éè®½åˆºæ ·æœ¬: {successful - sarcastic}")
        
        if successful > 0:
            accuracy = sum(1 for r in results if 'error' not in r and 'true_label' in r and 
                          r.get('is_sarcastic') == (r.get('true_label') == 1)) / successful
            print(f"ğŸ“Š å‡†ç¡®ç‡: {accuracy:.4f}")
        
        print("=" * 60)
        
    except Exception as e:
        print(f"\nğŸ’¥ é¢„æµ‹å¤±è´¥: {str(e)}")
        print("è¯·æ£€æŸ¥ä»¥ä¸‹å¯èƒ½çš„é—®é¢˜:")
        print("1. æ¨¡å‹æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("2. æµ‹è¯•æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        print("3. æ•°æ®ç›®å½•ç»“æ„æ˜¯å¦ä¸è®­ç»ƒæ—¶ä¸€è‡´")